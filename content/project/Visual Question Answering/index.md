---
title: Visual Question Answering
summary: This project compares the performance of a base and fine-tuned BLIP model for Visual Question Answering using the VizWiz dataset, demonstrating the improvements from task-specific training.

tags:
  - MultiModal
date: '2023-07-27T00:00:00Z'

# Optional external URL for project (replaces project detail page).
external_link: ''

image:
  caption: ""
  focal_point: Smart

links:
  # - icon: github
  #   icon_pack: fab
  #   name: Follow
  #   url: https://github.com/Yasien99/GI-Tract-Image-Segmentation
url_code: 'https://github.com/Al-ameen007/Visual-Question-Answering'
url_pdf: ''
url_slides: ''
url_video: ''

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
# slides: example
---
# Visual-Question-Answering
This project evaluates the performance of the BLIP model for Visual Question Answering (VQA) using the VizWiz dataset by comparing a base pre-trained model with a fine-tuned one. The workflow involves loading the dataset, preparing the models, generating predictions, and visualizing the results. The fine-tuned model, specifically trained on VizWiz, shows improvements in accuracy compared to the base model. Visual and quantitative comparisons are used to highlight the effectiveness of task-specific training in VQA tasks.




